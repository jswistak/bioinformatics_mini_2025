{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein–Protein Interactions Laboratory\n",
    "## Student Notebook\n",
    "\n",
    "This notebook contains exercises for the Protein–Protein Interactions laboratory session. Complete each task by following the step-by-step instructions and filling in the code stubs.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Retrieve protein-protein interaction data from public databases\n",
    "- Build and analyze protein interaction networks\n",
    "- Apply graph theory concepts to biological systems\n",
    "- Understand network robustness and biological implications\n",
    "\n",
    "**Instructions:**\n",
    "- Read each task description carefully\n",
    "- Follow the step-by-step instructions\n",
    "- Fill in the code stubs marked with `# TODO:` comments\n",
    "- Run each cell and verify your results\n",
    "- Ask for help if you get stuck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Retrieve PPI Data from STRING API\n",
    "\n",
    "**Aim:** Retrieve first-neighbor protein-protein interactions (PPIs) from the STRING database for two proteins: **PARG** (Poly(ADP-ribose) glycohydrolase) and **TP53** (p53 tumor suppressor). Compare their interaction networks by analyzing the interaction scores.\n",
    "\n",
    "**Background:** STRING (Search Tool for the Retrieval of Interacting Genes/Proteins) is a database of known and predicted protein-protein interactions. It integrates data from multiple sources including experimental evidence, computational predictions, and text mining. First neighbors are proteins that directly interact with the query protein.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. Implement the `string_first_neighbors` function to query STRING API\n",
    "2. Retrieve interactions for both PARG and TP53\n",
    "3. Create DataFrames for each protein's interactions\n",
    "4. Calculate and display statistics (mean, max, min) for interaction scores\n",
    "5. Compare the two networks\n",
    "\n",
    "**Key Concepts:**\n",
    "- REST API usage for biological databases\n",
    "- Protein identifiers and species taxonomy IDs (9606 = human)\n",
    "- Interaction confidence scores\n",
    "- Dataframe manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement string_first_neighbors function\n",
    "def string_first_neighbors(protein, species=9606, min_score=0.7):\n",
    "    \"\"\"\n",
    "    Retrieve first neighbors of a protein using STRING interactors API.\n",
    "    \n",
    "    Parameters:\n",
    "    - protein: protein name (e.g., \"TP53\")\n",
    "    - species: NCBI taxonomy ID (9606 = human)\n",
    "    - min_score: minimum confidence score threshold\n",
    "    \n",
    "    Returns:\n",
    "    - list of tuples: [(protein1, protein2, score), ...]\n",
    "    \n",
    "    Hint: Use the STRING API endpoint:\n",
    "    https://string-db.org/api/json/interaction_partners?identifier={protein}&species={species}\n",
    "    \"\"\"\n",
    "    # TODO: Construct the API URL\n",
    "    url = None  # Fill in the URL\n",
    "    \n",
    "    # TODO: Make the API request\n",
    "    response = None  # Use requests.get()\n",
    "    \n",
    "    # TODO: Check if request was successful\n",
    "    # Use response.ok or response.raise_for_status()\n",
    "    \n",
    "    # TODO: Parse JSON response\n",
    "    data = None  # Use response.json()\n",
    "    \n",
    "    # TODO: Extract interactions as list of tuples (protein, partner, score)\n",
    "    # Filter by min_score threshold\n",
    "    edges = []  # Fill in the list comprehension\n",
    "    \n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Retrieve PPI data from STRING API for PARG and TP53\n",
    "# TODO: Set up parameters\n",
    "proteins = None  # List of proteins to query\n",
    "species_id = None  # Human species ID\n",
    "min_score = None  # Minimum confidence score threshold\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 1: Retrieving First Neighbors from STRING Database\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Create dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# TODO: Loop through each protein\n",
    "for protein in proteins:\n",
    "    print(f\"\\nFetching STRING first neighbors for {protein}...\")\n",
    "    \n",
    "    try:\n",
    "        # TODO: Get interactions using string_first_neighbors function\n",
    "        interactions = None\n",
    "        \n",
    "        # TODO: Create DataFrame with columns: ['Protein_A', 'Protein_B', 'Score']\n",
    "        df = None\n",
    "        \n",
    "        # TODO: Store DataFrame in dataframes dictionary\n",
    "        dataframes[protein] = None\n",
    "        \n",
    "        # TODO: Print summary information\n",
    "        print(f\"✓ Retrieved {len(df)} interactions for {protein}\")\n",
    "        print(f\"  First 3 interactions:\")\n",
    "        # TODO: Display first 3 interactions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        # Create empty dataframe as fallback\n",
    "        dataframes[protein] = pd.DataFrame(columns=['Protein_A', 'Protein_B', 'Score'])\n",
    "\n",
    "# TODO: Display statistics for each protein\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Interaction Score Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for protein in proteins:\n",
    "    df = dataframes[protein]\n",
    "    if len(df) > 0:\n",
    "        scores = df['Score']\n",
    "        print(f\"\\n{protein} ({len(df)} interactions):\")\n",
    "        # TODO: Calculate and print mean, max, min scores\n",
    "        print(f\"  Mean score: {None:.3f}\")  # Fill in\n",
    "        print(f\"  Max score:  {None:.3f}\")  # Fill in\n",
    "        print(f\"  Min score:  {None:.3f}\")  # Fill in\n",
    "    else:\n",
    "        print(f\"\\n{protein}: No interactions found\")\n",
    "\n",
    "# TODO: Display DataFrames (first 10 rows)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DataFrames Summary\")\n",
    "print(\"=\" * 60)\n",
    "# TODO: Print head of each dataframe\n",
    "\n",
    "# TODO: Compare the two networks\n",
    "# TODO: Find common interactors between PARG and TP53 networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build n-Hop Neighborhood Graphs using BFS\n",
    "\n",
    "**Aim:** Use Breadth-First Search (BFS) to find all proteins within n hops (default n=2) from the target proteins (PARG and TP53), and also retrieve all interactions between these discovered proteins. This creates a complete interaction subgraph centered on each target protein.\n",
    "\n",
    "**Background:** A protein-centered network includes not only direct interactors but also their interactions with each other. This provides a more complete view of the local interaction neighborhood. BFS systematically explores the network layer by layer, ensuring we capture all proteins within the specified distance.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. Implement `string_n_hop_neighbors` function using BFS\n",
    "2. Step 1: Use BFS to discover all nodes within n hops\n",
    "3. Step 2: For each discovered node, get all its first neighbors\n",
    "4. Step 3: Filter to keep only edges between discovered nodes\n",
    "5. Build NetworkX graphs for both PARG and TP53\n",
    "6. Display graph statistics\n",
    "\n",
    "**Key Concepts:**\n",
    "- Breadth-First Search (BFS) algorithm\n",
    "- n-hop neighborhood discovery\n",
    "- Building complete subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement string_n_hop_neighbors function\n",
    "def string_n_hop_neighbors(protein, n=2, species=9606, min_score=0.7):\n",
    "    \"\"\"\n",
    "    Retrieve nodes and edges up to n hops away from the input protein using BFS.\n",
    "    Also retrieves all interactions between the discovered nodes.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Use BFS to discover all nodes within n hops\n",
    "    2. For each discovered node, get all its first neighbors\n",
    "    3. Filter to keep only edges between discovered nodes\n",
    "    4. Return all edges in the subgraph\n",
    "    \n",
    "    Returns: list of tuples [(protein1, protein2, score), ...]\n",
    "    \"\"\"\n",
    "    # Step 1: BFS to discover all nodes within n hops\n",
    "    frontier = {protein}  # Starting set\n",
    "    visited = {protein}   # All discovered nodes\n",
    "    \n",
    "    # TODO: Implement BFS loop for n hops\n",
    "    \n",
    "    \n",
    "    # Step 2: Get all interactions between discovered nodes\n",
    "    all_edges = []\n",
    "    discovered_nodes = list(visited)\n",
    "    \n",
    "    print(f\"  Discovered {len(discovered_nodes)} nodes, fetching all interactions...\")\n",
    "    \n",
    "    # TODO: Fetch interactions for each discovered node\n",
    "    \n",
    "    # TODO: Remove duplicate edges if any (keep first occurrence)\n",
    "    # Hint: Use a set to track seen edges (use sorted tuple as key)\n",
    "    seen = set()\n",
    "    unique_edges = []\n",
    "    # TODO: Filter duplicates\n",
    "    \n",
    "    print(f\"  Retrieved {len(unique_edges)} unique interactions\")\n",
    "    return unique_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Build n-hop neighborhood graphs for PARG and TP53\n",
    "# TODO: Set parameters\n",
    "n_hops = None  # Number of hops (default: 2)\n",
    "species_id = None  # Human species ID\n",
    "min_score = None  # Minimum confidence score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 2: Building n-Hop Neighborhood Graphs (BFS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Create dictionary to store graphs\n",
    "graphs = {}\n",
    "\n",
    "# TODO: Loop through each protein\n",
    "for protein in [\"PARG\", \"TP53\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {protein} (n={n_hops} hops)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # TODO: Get all edges in the n-hop neighborhood\n",
    "        edges = None\n",
    "        \n",
    "        # TODO: Build NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        # TODO: Add edges to graph (with weight=score)\n",
    "        \n",
    "        # TODO: Store graph\n",
    "        graphs[protein] = None\n",
    "        \n",
    "        # TODO: Print graph statistics\n",
    "        print(f\"\\n✓ Graph constructed for {protein}:\")\n",
    "        print(f\"  Nodes: {None}\")  # Fill in\n",
    "        print(f\"  Edges: {None}\")  # Fill in\n",
    "        print(f\"  Density: {None:.4f}\")  # Fill in\n",
    "        print(f\"  Is connected: {None}\")  # Fill in\n",
    "        \n",
    "        # TODO: If not connected, print component information\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error building graph for {protein}: {e}\")\n",
    "        graphs[protein] = nx.Graph()  # Empty graph as fallback\n",
    "\n",
    "# TODO: Compare the two graphs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Graph Comparison\")\n",
    "print(\"=\" * 60)\n",
    "# TODO: Print comparison statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualize PPI Networks\n",
    "\n",
    "**Aim:** Visualize the n-hop neighborhood graphs created in Task 2 for PARG and TP53. Create publication-quality network visualizations with proper node highlighting, edge styling, and layout algorithms.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. Check if graphs exist from Task 2\n",
    "2. For each graph, create a network visualization with:\n",
    "   - Target protein highlighted in red\n",
    "   - Other nodes in light blue\n",
    "   - Edge widths based on interaction scores\n",
    "   - Node labels\n",
    "3. Create degree distribution plots\n",
    "4. Display all visualizations inline\n",
    "\n",
    "**Key Concepts:**\n",
    "- Network visualization techniques\n",
    "- Layout algorithms (spring layout)\n",
    "- Node and edge styling\n",
    "- Highlighting important nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Visualize the n-hop neighborhood graphs\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 3: Visualizing PPI Networks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Visualizing {protein} network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        \n",
    "        # TODO: Create visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # TODO: Use spring layout (seed=42 for reproducibility)\n",
    "        pos = None\n",
    "        \n",
    "        # TODO: Create node colors (red for target protein, lightblue for others)\n",
    "        node_colors = None\n",
    "        \n",
    "        # TODO: Create node sizes (larger for target protein)\n",
    "        node_sizes = None\n",
    "        \n",
    "        # TODO: Draw nodes\n",
    "        # Use nx.draw_networkx_nodes()\n",
    "        \n",
    "        # TODO: Draw edges (optionally weight by score)\n",
    "        # Get edges with data: G.edges(data=True)\n",
    "        # Calculate edge widths based on weight\n",
    "        edge_widths = None  # Calculate from edge weights\n",
    "        # Use nx.draw_networkx_edges()\n",
    "        \n",
    "        # TODO: Draw labels\n",
    "        # Use nx.draw_networkx_labels()\n",
    "        \n",
    "        # TODO: Set title and axis\n",
    "        plt.title(f\"{protein} {n_hops}-Hop Neighborhood Network\\n\"\n",
    "                 f\"({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)\",\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # TODO: Additional visualization: Degree distribution\n",
    "        # Calculate degrees\n",
    "        degrees = None  # Use dict(G.degree())\n",
    "        degree_values = None  # Extract values\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # TODO: Plot degree distribution histogram\n",
    "        # Use ax1.hist()\n",
    "        \n",
    "        # TODO: Plot top 10 nodes by degree\n",
    "        # Sort degrees, get top 10, create bar plot on ax2\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Visualization Complete\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Graph Analysis and Centrality Measures\n",
    "\n",
    "**Aim:** Perform comprehensive graph analysis for both PARG and TP53 networks. Calculate basic graph statistics (number of nodes, connected components) and compute centrality measures (degree, betweenness, closeness) to identify key hub proteins. Visualize the networks with top-centrality nodes highlighted.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. For each graph, calculate basic statistics:\n",
    "   - Number of nodes and edges\n",
    "   - Number of connected components\n",
    "   - Whether the graph is connected\n",
    "2. Calculate centrality measures:\n",
    "   - Degree centrality\n",
    "   - Betweenness centrality\n",
    "   - Closeness centrality\n",
    "3. Create bar plots showing top 10 nodes for each centrality measure\n",
    "4. Visualize networks with top 5 nodes by each centrality highlighted\n",
    "5. Compare results between PARG and TP53\n",
    "\n",
    "**Key Concepts:**\n",
    "- Basic graph statistics\n",
    "- Centrality measures and their interpretation\n",
    "- Network visualization with node highlighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Graph Analysis and Centrality Measures\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 4: Graph Analysis and Centrality Measures\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # TODO: Store results for both graphs\n",
    "    analysis_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # TODO: Calculate basic graph statistics\n",
    "        num_nodes = None  # Use G.number_of_nodes()\n",
    "        num_edges = None  # Use G.number_of_edges()\n",
    "        components = None  # Use nx.connected_components(G)\n",
    "        num_components = None  # Count components\n",
    "        is_connected = None  # Use nx.is_connected(G)\n",
    "        \n",
    "        print(f\"\\nBasic Graph Statistics:\")\n",
    "        print(f\"  Number of nodes: {num_nodes}\")\n",
    "        print(f\"  Number of edges: {num_edges}\")\n",
    "        print(f\"  Number of connected components: {num_components}\")\n",
    "        print(f\"  Is connected: {is_connected}\")\n",
    "        # TODO: If not connected, print largest component size\n",
    "        \n",
    "        # TODO: Calculate centrality measures\n",
    "        print(f\"\\nComputing centrality measures...\")\n",
    "        degree_cent = None  # Use nx.degree_centrality(G)\n",
    "        betweenness_cent = None  # Use nx.betweenness_centrality(G)\n",
    "        closeness_cent = None  # Use nx.closeness_centrality(G)\n",
    "        \n",
    "        # TODO: Create DataFrame from centrality measures\n",
    "        centrality_df = None  # Use pd.DataFrame()\n",
    "        # TODO: Sort by degree centrality\n",
    "        \n",
    "        # TODO: Store results\n",
    "        analysis_results[protein] = {\n",
    "            'graph': G,\n",
    "            'centrality_df': centrality_df,\n",
    "            'stats': {\n",
    "                'nodes': num_nodes,\n",
    "                'edges': num_edges,\n",
    "                'components': num_components,\n",
    "                'is_connected': is_connected\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Centrality measures computed\")\n",
    "        print(f\"\\nTop 10 hub proteins (by degree centrality):\")\n",
    "        # TODO: Print top 10\n",
    "        \n",
    "        # TODO: Visualize centrality measures with bar plots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # TODO: Plot top 10 by degree centrality on axes[0]\n",
    "        # TODO: Plot top 10 by betweenness centrality on axes[1]\n",
    "        # TODO: Plot top 10 by closeness centrality on axes[2]\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # TODO: Correlation between centrality measures\n",
    "        print(f\"\\nCorrelation between centrality measures ({protein}):\")\n",
    "        # TODO: Calculate and print correlation matrix\n",
    "        \n",
    "        # TODO: Visualize network with top-centrality nodes highlighted\n",
    "        print(f\"\\nVisualizing network with top-centrality nodes highlighted...\")\n",
    "        \n",
    "        # TODO: Get top 5 nodes for each centrality measure\n",
    "        top_degree_nodes = None\n",
    "        top_between_nodes = None\n",
    "        top_close_nodes = None\n",
    "        \n",
    "        # TODO: Create three visualizations, one for each centrality measure\n",
    "        # Use subplots(1, 3) and highlight nodes in different colors\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # TODO: Print top nodes for each centrality\n",
    "        print(f\"\\nTop 5 nodes by centrality ({protein}):\")\n",
    "        # TODO: Print lists\n",
    "    \n",
    "    # TODO: Compare results between PARG and TP53\n",
    "    if len(analysis_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: PARG vs TP53\")\n",
    "        print(\"=\" * 60)\n",
    "        # TODO: Compare graph sizes, connectivity, average centrality values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Community Detection (Louvain Clustering)\n",
    "\n",
    "**Aim:** Identify functional communities/modules in both PARG and TP53 networks using the Louvain algorithm. Compare the community structure between the two networks.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. Check if python-louvain is available (import community.community_louvain)\n",
    "2. For each graph:\n",
    "   - Apply Louvain clustering to find communities\n",
    "   - Calculate modularity score\n",
    "   - Organize proteins by cluster\n",
    "   - Display community sizes\n",
    "3. Visualize networks with nodes colored by community\n",
    "4. Compare community structures between PARG and TP53\n",
    "\n",
    "**Key Concepts:**\n",
    "- Community detection algorithms\n",
    "- Modularity optimization\n",
    "- Network partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Community Detection using Louvain algorithm\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 5: Community Detection (Louvain Clustering)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Check if Louvain is available\n",
    "try:\n",
    "    import community.community_louvain as community_louvain\n",
    "    louvain_available = True\n",
    "except ImportError:\n",
    "    louvain_available = False\n",
    "    print(\"⚠ python-louvain not installed. Using NetworkX alternative method...\")\n",
    "\n",
    "# TODO: Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # TODO: Store clustering results\n",
    "    clustering_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Community Detection for {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if louvain_available:\n",
    "            print(\"Performing Louvain clustering...\")\n",
    "            \n",
    "            # TODO: Compute best partition\n",
    "            partition = None  # Use community_louvain.best_partition(G)\n",
    "            \n",
    "            # TODO: Organize proteins by cluster\n",
    "            clusters = defaultdict(list)\n",
    "            # TODO: Fill clusters dictionary\n",
    "            \n",
    "            print(f\"✓ Identified {len(clusters)} communities\")\n",
    "            print(f\"\\nCommunity sizes:\")\n",
    "            # TODO: Print community sizes and members (for small clusters)\n",
    "            \n",
    "            # TODO: Calculate modularity\n",
    "            modularity = None  # Use community_louvain.modularity(partition, G)\n",
    "            print(f\"\\nNetwork modularity: {modularity:.4f}\")\n",
    "            \n",
    "            clustering_results[protein] = {\n",
    "                'partition': partition,\n",
    "                'clusters': clusters,\n",
    "                'modularity': modularity\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # TODO: Fallback: Use NetworkX built-in community detection\n",
    "            print(\"Using NetworkX greedy modularity communities...\")\n",
    "            communities = None  # Use nx.community.greedy_modularity_communities(G)\n",
    "            \n",
    "            # TODO: Create partition dict and calculate modularity\n",
    "            partition = {}\n",
    "            # TODO: Fill partition dictionary\n",
    "            \n",
    "            modularity = None  # Use nx.community.modularity(G, communities)\n",
    "            \n",
    "            # TODO: Convert to clusters dict format\n",
    "            clusters = defaultdict(list)\n",
    "            # TODO: Fill clusters dictionary\n",
    "            \n",
    "            clustering_results[protein] = {\n",
    "                'partition': partition,\n",
    "                'clusters': clusters,\n",
    "                'modularity': modularity\n",
    "            }\n",
    "        \n",
    "        # TODO: Visualize network with communities\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # TODO: Use consistent layout\n",
    "        pos = None\n",
    "        \n",
    "        # TODO: Color nodes by community\n",
    "        node_colors = None  # List of cluster IDs\n",
    "        cmap = plt.cm.tab20\n",
    "        \n",
    "        # TODO: Highlight target protein\n",
    "        node_sizes = None\n",
    "        node_edge_colors = None\n",
    "        node_linewidths = None\n",
    "        \n",
    "        # TODO: Draw network\n",
    "        # Use nx.draw_networkx_nodes(), nx.draw_networkx_edges(), nx.draw_networkx_labels()\n",
    "        \n",
    "        plt.title(f\"{protein} Network with Communities (Louvain, {len(clusters)} clusters, modularity={modularity:.3f})\", \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # TODO: Compare community structures between PARG and TP53\n",
    "    if len(clustering_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: PARG vs TP53 Communities\")\n",
    "        print(\"=\" * 60)\n",
    "        # TODO: Compare number of communities, modularity, community sizes, common nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Functional Enrichment Analysis (g:Profiler)\n",
    "\n",
    "**Aim:** Query g:Profiler API to identify enriched Gene Ontology (GO) terms and KEGG pathways for proteins in both PARG and TP53 networks. Compare the functional enrichment between the two networks to understand their biological roles.\n",
    "\n",
    "**Background:** Functional enrichment analysis identifies which biological processes, molecular functions, cellular components, or pathways are overrepresented in a set of genes/proteins compared to the background. g:Profiler is a web-based tool that performs enrichment analysis using multiple databases including:\n",
    "- **GO:BP** (Gene Ontology Biological Process): Biological processes proteins are involved in\n",
    "- **GO:MF** (Gene Ontology Molecular Function): Molecular functions proteins perform\n",
    "- **GO:CC** (Gene Ontology Cellular Component): Cellular locations where proteins are found\n",
    "- **KEGG**: Kyoto Encyclopedia of Genes and Genomes pathways\n",
    "- **REAC**: Reactome pathways\n",
    "\n",
    "**Biological Significance:** Enrichment analysis helps interpret network data by:\n",
    "- Identifying common biological functions of interacting proteins\n",
    "- Revealing pathways that are overrepresented in the network\n",
    "- Understanding the biological context of protein interactions\n",
    "- Comparing functional profiles between different networks\n",
    "\n",
    "**Key Concepts:**\n",
    "- Functional enrichment analysis\n",
    "- Statistical significance (p-values, FDR correction)\n",
    "- Gene Ontology and pathway databases\n",
    "- Interpreting enrichment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Functional Enrichment Analysis using g:Profiler\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 6: Functional Enrichment Analysis (g:Profiler)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # TODO: Store enrichment results\n",
    "    enrichment_results = {}\n",
    "    \n",
    "    # TODO: Query g:Profiler API\n",
    "    url = \"https://biit.cs.ut.ee/gprofiler/api/gost/profile/\"\n",
    "    \n",
    "    # Color map for different sources\n",
    "    source_colors = {\n",
    "        'GO:BP': 'steelblue',\n",
    "        'GO:MF': 'coral',\n",
    "        'GO:CC': 'mediumseagreen',\n",
    "        'KEGG': 'purple',\n",
    "        'REAC': 'orange'\n",
    "    }\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        # TODO: Get list of all protein nodes\n",
    "        genes = None\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Enrichment Analysis for {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Querying g:Profiler for {len(genes)} genes...\")\n",
    "        \n",
    "        # TODO: Prepare API payload\n",
    "        payload = {\n",
    "            \"organism\": \"hsapiens\",\n",
    "            \"query\": None,  # Fill in genes list\n",
    "            \"sources\": [\"GO:BP\", \"GO:MF\", \"GO:CC\", \"KEGG\", \"REAC\"],\n",
    "            \"user_threshold\": None,  # P-value threshold (e.g., 0.05)\n",
    "            \"all_results\": False,\n",
    "            \"ordered\": True\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # TODO: Make API request\n",
    "            response = None  # Use requests.post()\n",
    "            \n",
    "            # TODO: Check response and parse JSON\n",
    "            results = None\n",
    "            \n",
    "            # TODO: Parse and display top enriched terms\n",
    "            if 'result' in results and len(results['result']) > 0:\n",
    "                enrichment_df = None  # Create DataFrame from results['result']\n",
    "                \n",
    "                # TODO: Filter by p-value and sort\n",
    "                enrichment_df = None  # Filter p_value < 0.05 and sort\n",
    "                \n",
    "                enrichment_results[protein] = enrichment_df\n",
    "                \n",
    "                print(f\"✓ Found {len(enrichment_df)} significantly enriched terms (p < 0.05)\")\n",
    "                print(f\"\\nTop 10 enriched terms:\")\n",
    "                # TODO: Display top 10 terms\n",
    "                \n",
    "                # TODO: Visualize top enriched terms\n",
    "                top_terms = None  # Get top 15\n",
    "                \n",
    "                if len(top_terms) > 0:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    \n",
    "                    # TODO: Create bar plot with -log10(p-value)\n",
    "                    # Color bars by source\n",
    "                    # Add legend\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                \n",
    "                # TODO: Breakdown by source\n",
    "                print(f\"\\nEnrichment by source ({protein}):\")\n",
    "                # TODO: Count terms by source\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ No enrichment results found\")\n",
    "                enrichment_results[protein] = pd.DataFrame()\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"✗ Error querying g:Profiler: {e}\")\n",
    "            enrichment_results[protein] = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing enrichment results: {e}\")\n",
    "            enrichment_results[protein] = pd.DataFrame()\n",
    "    \n",
    "    # TODO: Compare enrichment between PARG and TP53\n",
    "    if len(enrichment_results) == 2:\n",
    "        parg_df = enrichment_results['PARG']\n",
    "        tp53_df = enrichment_results['TP53']\n",
    "        \n",
    "        if len(parg_df) > 0 and len(tp53_df) > 0:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"Comparison: PARG vs TP53 Enrichment\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # TODO: Compare number of enriched terms\n",
    "            # TODO: Find common enriched terms\n",
    "            # TODO: Compare by source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Simulated Network Attack (Protein Inhibition)\n",
    "\n",
    "**Aim:** Simulate the effect of inhibiting the central protein (TP53 or PARG) by removing it from each network. Compare graph properties before and after removal to understand the impact of targeted protein inhibition on network structure and connectivity.\n",
    "\n",
    "**Steps to complete:**\n",
    "1. For each graph, calculate metrics BEFORE removal:\n",
    "   - Number of nodes and edges\n",
    "   - Network density\n",
    "   - Connectivity status\n",
    "   - Number of connected components\n",
    "   - Largest component size\n",
    "   - Average degree, clustering, path length\n",
    "2. Remove the central protein from the graph\n",
    "3. Calculate the same metrics AFTER removal\n",
    "4. Calculate impact metrics:\n",
    "   - Nodes/edges lost\n",
    "   - Largest component retention\n",
    "   - Network fragmentation\n",
    "   - Connectivity loss\n",
    "5. Visualize before and after networks\n",
    "6. Create bar chart comparing metrics\n",
    "7. Compare impact between PARG and TP53 removal\n",
    "\n",
    "**Key Concepts:**\n",
    "- Network robustness and vulnerability\n",
    "- Targeted node removal\n",
    "- Graph connectivity and fragmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Simulated Network Attack (Removing Central Proteins)\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 7: Simulated Network Attack (Protein Inhibition)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # TODO: Store attack results\n",
    "    attack_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Simulating Attack: Removing {protein} from Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # TODO: Calculate metrics BEFORE removal\n",
    "        print(f\"\\nBEFORE removal of {protein}:\")\n",
    "        before_metrics = {\n",
    "            'nodes': None,  # Use G.number_of_nodes()\n",
    "            'edges': None,  # Use G.number_of_edges()\n",
    "            'density': None,  # Use nx.density(G)\n",
    "            'is_connected': None,  # Use nx.is_connected(G)\n",
    "            'components': None,  # Count connected components\n",
    "            'largest_component': None,  # Size of largest component\n",
    "            'avg_degree': None,  # Calculate from degrees\n",
    "            'clustering': None,  # Use nx.average_clustering(G)\n",
    "        }\n",
    "        \n",
    "        # TODO: If connected, calculate path length and diameter\n",
    "        if before_metrics['is_connected']:\n",
    "            before_metrics['avg_path_length'] = None  # Use nx.average_shortest_path_length(G)\n",
    "            before_metrics['diameter'] = None  # Use nx.diameter(G)\n",
    "        else:\n",
    "            before_metrics['avg_path_length'] = None\n",
    "            before_metrics['diameter'] = None\n",
    "        \n",
    "        # TODO: Print before metrics\n",
    "        \n",
    "        # TODO: Remove the central protein\n",
    "        G_attacked = G.copy()\n",
    "        if protein in G_attacked.nodes():\n",
    "            # TODO: Remove the node\n",
    "            pass\n",
    "            print(f\"\\n✓ Removed {protein} from network\")\n",
    "        \n",
    "        # TODO: Calculate metrics AFTER removal\n",
    "        print(f\"\\nAFTER removal of {protein}:\")\n",
    "        after_metrics = {\n",
    "            'nodes': None,  # Calculate from G_attacked\n",
    "            'edges': None,\n",
    "            'density': None,\n",
    "            'is_connected': None,\n",
    "            'components': None,\n",
    "            'largest_component': None,\n",
    "            'avg_degree': None,\n",
    "            'clustering': None,\n",
    "        }\n",
    "        \n",
    "        # TODO: If connected after removal, calculate path length and diameter\n",
    "        \n",
    "        # TODO: Print after metrics with changes\n",
    "        \n",
    "        # TODO: Calculate impact metrics\n",
    "        impact = {\n",
    "            'nodes_lost': None,  # Calculate difference\n",
    "            'edges_lost': None,\n",
    "            'largest_component_loss': None,\n",
    "            'largest_component_retention': None,  # Percentage\n",
    "            'fragmentation': None,  # Additional components\n",
    "            'connectivity_lost': None  # Boolean: was connected, now not\n",
    "        }\n",
    "        \n",
    "        # TODO: Print impact summary\n",
    "        \n",
    "        attack_results[protein] = {\n",
    "            'before': before_metrics,\n",
    "            'after': after_metrics,\n",
    "            'impact': impact,\n",
    "            'graph_attacked': G_attacked\n",
    "        }\n",
    "        \n",
    "        # TODO: Visualize before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # TODO: Plot network BEFORE removal (highlight target protein in red)\n",
    "        # TODO: Plot network AFTER removal\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # TODO: Bar plot comparing metrics\n",
    "        # Compare: nodes, edges, density, avg_degree, clustering\n",
    "        # Use side-by-side bars (before vs after)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # TODO: Compare impact between PARG and TP53 removal\n",
    "    if len(attack_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: Impact of Removing PARG vs TP53\")\n",
    "        print(\"=\" * 60)\n",
    "        # TODO: Compare fragmentation, component retention, connectivity loss, edges lost\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
