{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein–Protein Interactions Laboratory\n",
    "## Teacher's Solution Notebook\n",
    "\n",
    "This notebook contains fully solved exercises for the Protein–Protein Interactions laboratory session. Each task demonstrates practical bioinformatics analysis using real biological data and APIs.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Retrieve protein-protein interaction data from public databases\n",
    "- Build and analyze protein interaction networks\n",
    "- Apply graph theory concepts to biological systems\n",
    "- Use machine learning for interaction prediction\n",
    "- Understand network robustness and biological implications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Retrieve PPI Data from STRING API\n",
    "\n",
    "**Aim:** Retrieve first-neighbor protein-protein interactions (PPIs) from the STRING database for two proteins: **PARG** (Poly(ADP-ribose) glycohydrolase) and **TP53** (p53 tumor suppressor). Compare their interaction networks by analyzing the interaction scores.\n",
    "\n",
    "**Background:** STRING (Search Tool for the Retrieval of Interacting Genes/Proteins) is a database of known and predicted protein-protein interactions. It integrates data from multiple sources including experimental evidence, computational predictions, and text mining. First neighbors are proteins that directly interact with the query protein.\n",
    "\n",
    "**Key Concepts:**\n",
    "- REST API usage for biological databases\n",
    "- Protein identifiers and species taxonomy IDs\n",
    "- Interaction confidence scores\n",
    "- Dataframe manipulation for comparing datasets\n",
    "- Statistical analysis of interaction scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_first_neighbors(protein, species=9606, min_score=0.7):\n",
    "    \"\"\"\n",
    "    Retrieve first neighbors of a protein using STRING interactors API.\n",
    "    Returns list of (protein1, protein2, score)\n",
    "    \"\"\"\n",
    "    url = f\"https://string-db.org/api/json/interaction_partners?identifier={protein}&species={species}\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        raise RuntimeError(f\"STRING API error: {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    edges = [(protein, e[\"preferredName_B\"], e[\"score\"]) for e in data if e[\"score\"] >= min_score]\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Retrieve PPI data from STRING API for PARG and TP53\n",
    "# Species ID 9606 = Homo sapiens (human)\n",
    "\n",
    "proteins = [\"PARG\", \"TP53\"]\n",
    "species_id = 9606  # Human\n",
    "min_score = 0.7  # Minimum confidence score threshold\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 1: Retrieving First Neighbors from STRING Database\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Retrieve interactions for both proteins\n",
    "dataframes = {}\n",
    "\n",
    "for protein in proteins:\n",
    "    print(f\"\\nFetching STRING first neighbors for {protein}...\")\n",
    "    \n",
    "    try:\n",
    "        # Use string_first_neighbors function\n",
    "        interactions = string_first_neighbors(protein, species=species_id, min_score=min_score)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(interactions, columns=['Protein_A', 'Protein_B', 'Score'])\n",
    "        dataframes[protein] = df\n",
    "        \n",
    "        print(f\"✓ Retrieved {len(df)} interactions for {protein}\")\n",
    "        print(f\"  First 3 interactions:\")\n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"    {row['Protein_A']} ↔ {row['Protein_B']} (score: {row['Score']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error fetching data for {protein}: {e}\")\n",
    "        # Create empty dataframe as fallback\n",
    "        dataframes[protein] = pd.DataFrame(columns=['Protein_A', 'Protein_B', 'Score'])\n",
    "\n",
    "# Display statistics for each protein\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Interaction Score Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for protein in proteins:\n",
    "    df = dataframes[protein]\n",
    "    if len(df) > 0:\n",
    "        scores = df['Score']\n",
    "        print(f\"\\n{protein} ({len(df)} interactions):\")\n",
    "        print(f\"  Mean score: {scores.mean():.3f}\")\n",
    "        print(f\"  Max score:  {scores.max():.3f}\")\n",
    "        print(f\"  Min score:  {scores.min():.3f}\")\n",
    "        print(f\"  Median:     {scores.median():.3f}\")\n",
    "        print(f\"  Std dev:    {scores.std():.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n{protein}: No interactions found (or error occurred)\")\n",
    "\n",
    "# Display dataframes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DataFrames Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for protein in proteins:\n",
    "    df = dataframes[protein]\n",
    "    print(f\"\\n{protein} DataFrame:\")\n",
    "    print(df.head(10).to_string(index=False))\n",
    "    if len(df) > 10:\n",
    "        print(f\"... and {len(df) - 10} more rows\")\n",
    "\n",
    "# Compare the two networks\n",
    "if len(dataframes[\"PARG\"]) > 0 and len(dataframes[\"TP53\"]) > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Network Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PARG has {len(dataframes['PARG'])} first neighbors\")\n",
    "    print(f\"TP53 has {len(dataframes['TP53'])} first neighbors\")\n",
    "    \n",
    "    # Find common interactors\n",
    "    parg_partners = set(dataframes['PARG']['Protein_B'])\n",
    "    tp53_partners = set(dataframes['TP53']['Protein_B'])\n",
    "    common = parg_partners & tp53_partners\n",
    "    \n",
    "    print(f\"\\nCommon interactors: {len(common)}\")\n",
    "    if len(common) > 0:\n",
    "        print(f\"  Examples: {', '.join(list(common)[:5])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build n-Hop Neighborhood Graphs using BFS\n",
    "\n",
    "**Aim:** Use Breadth-First Search (BFS) to find all proteins within n hops (default n=2) from the target proteins (PARG and TP53), and also retrieve all interactions between these discovered proteins. This creates a complete interaction subgraph centered on each target protein.\n",
    "\n",
    "**Background:** A protein-centered network includes not only direct interactors but also their interactions with each other. This provides a more complete view of the local interaction neighborhood. BFS systematically explores the network layer by layer, ensuring we capture all proteins within the specified distance.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Breadth-First Search (BFS) algorithm for network exploration\n",
    "- n-hop neighborhood discovery\n",
    "- Building complete subgraphs (nodes + all edges between them)\n",
    "- Graph construction from interaction data\n",
    "- Network visualization with spring layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_n_hop_neighbors(\n",
    "    protein,\n",
    "    n=2,\n",
    "    species=9606,\n",
    "    min_score=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve nodes and edges up to n hops away from the input protein\n",
    "    using BFS (Breadth-First Search). Also retrieves all interactions\n",
    "    between the discovered nodes to build a complete subgraph.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Use BFS to discover all nodes within n hops\n",
    "    2. For each discovered node, get all its first neighbors\n",
    "    3. Filter to keep only edges between discovered nodes\n",
    "    4. Return all edges in the subgraph\n",
    "    \n",
    "    Returns edges in the same format as string_first_neighbors:\n",
    "        [(protein1, protein2, score), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: BFS to discover all nodes within n hops\n",
    "    frontier = {protein}\n",
    "    visited = {protein}\n",
    "    \n",
    "    for hop in range(n):\n",
    "        new_frontier = set()\n",
    "        \n",
    "        for p in frontier:\n",
    "            try:\n",
    "                # Get first neighbors\n",
    "                edges = string_first_neighbors(\n",
    "                    p,\n",
    "                    species=species,\n",
    "                    min_score=min_score\n",
    "                )\n",
    "                \n",
    "                # Add new neighbors to visited and next frontier\n",
    "                for (_, partner, _) in edges:\n",
    "                    if partner not in visited:\n",
    "                        new_frontier.add(partner)\n",
    "                        visited.add(partner)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not fetch neighbors for {p}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        frontier = new_frontier\n",
    "        if not frontier:\n",
    "            break\n",
    "    \n",
    "    # Step 2: Get all interactions between discovered nodes\n",
    "    all_edges = []\n",
    "    discovered_nodes = list(visited)\n",
    "    \n",
    "    print(f\"  Discovered {len(discovered_nodes)} nodes, fetching all interactions...\")\n",
    "    \n",
    "    # Fetch interactions for each discovered node\n",
    "    for i, node in enumerate(discovered_nodes):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"    Progress: {i+1}/{len(discovered_nodes)} nodes processed...\")\n",
    "        \n",
    "        try:\n",
    "            edges = string_first_neighbors(\n",
    "                node,\n",
    "                species=species,\n",
    "                min_score=min_score\n",
    "            )\n",
    "            \n",
    "            # Only keep edges where both nodes are in our discovered set\n",
    "            for (p1, p2, score) in edges:\n",
    "                if p2 in visited:  # Both nodes are in discovered set\n",
    "                    # Avoid duplicates by using sorted tuple\n",
    "                    edge_tuple = tuple(sorted([p1, p2]))\n",
    "                    all_edges.append((p1, p2, score))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not fetch neighbors for {node}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Remove duplicate edges (keep first occurrence)\n",
    "    seen = set()\n",
    "    unique_edges = []\n",
    "    for edge in all_edges:\n",
    "        edge_key = tuple(sorted([edge[0], edge[1]]))\n",
    "        if edge_key not in seen:\n",
    "            seen.add(edge_key)\n",
    "            unique_edges.append(edge)\n",
    "    \n",
    "    print(f\"  Retrieved {len(unique_edges)} unique interactions\")\n",
    "    return unique_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Build n-hop neighborhood graphs for PARG and TP53\n",
    "n_hops = 2  # Number of hops from target protein\n",
    "species_id = 9606  # Human\n",
    "min_score = 0.7  # Minimum confidence score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 2: Building n-Hop Neighborhood Graphs (BFS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store graphs for each protein\n",
    "graphs = {}\n",
    "\n",
    "for protein in [\"PARG\", \"TP53\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {protein} (n={n_hops} hops)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Get all edges in the n-hop neighborhood\n",
    "        edges = string_n_hop_neighbors(\n",
    "            protein,\n",
    "            n=n_hops,\n",
    "            species=species_id,\n",
    "            min_score=min_score\n",
    "        )\n",
    "        \n",
    "        # Build NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        for p1, p2, score in edges:\n",
    "            G.add_edge(p1, p2, weight=score)\n",
    "        \n",
    "        graphs[protein] = G\n",
    "        \n",
    "        print(f\"\\n✓ Graph constructed for {protein}:\")\n",
    "        print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "        print(f\"  Edges: {G.number_of_edges()}\")\n",
    "        print(f\"  Density: {nx.density(G):.4f}\")\n",
    "        print(f\"  Is connected: {nx.is_connected(G)}\")\n",
    "        \n",
    "        if not nx.is_connected(G):\n",
    "            components = list(nx.connected_components(G))\n",
    "            print(f\"  Connected components: {len(components)}\")\n",
    "            print(f\"  Largest component: {len(max(components, key=len))} nodes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error building graph for {protein}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        graphs[protein] = nx.Graph()  # Empty graph as fallback\n",
    "\n",
    "# Compare the two graphs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Graph Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(graphs[\"PARG\"].nodes()) > 0 and len(graphs[\"TP53\"].nodes()) > 0:\n",
    "    print(f\"\\nPARG graph: {graphs['PARG'].number_of_nodes()} nodes, {graphs['PARG'].number_of_edges()} edges\")\n",
    "    print(f\"TP53 graph: {graphs['TP53'].number_of_nodes()} nodes, {graphs['TP53'].number_of_edges()} edges\")\n",
    "    \n",
    "    # Find common nodes\n",
    "    parg_nodes = set(graphs['PARG'].nodes())\n",
    "    tp53_nodes = set(graphs['TP53'].nodes())\n",
    "    common_nodes = parg_nodes & tp53_nodes\n",
    "    \n",
    "    print(f\"\\nCommon nodes: {len(common_nodes)}\")\n",
    "    if len(common_nodes) > 0:\n",
    "        print(f\"  Examples: {', '.join(list(common_nodes)[:10])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualize PPI Networks\n",
    "\n",
    "**Aim:** Visualize the n-hop neighborhood graphs created in Task 2 for PARG and TP53. Create publication-quality network visualizations with proper node highlighting, edge styling, and layout algorithms.\n",
    "\n",
    "**Background:** Network visualization is crucial for understanding the structure and organization of protein interaction networks. Different layout algorithms (spring, circular, hierarchical) can reveal different aspects of network topology. Visual elements like node size, color, and edge thickness can encode additional information.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Network visualization techniques\n",
    "- Layout algorithms (spring, circular, hierarchical)\n",
    "- Node and edge styling\n",
    "- Highlighting important nodes (e.g., target proteins)\n",
    "- Saving publication-quality figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Visualize the n-hop neighborhood graphs\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 3: Visualizing PPI Networks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Visualizing {protein} network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Use spring layout for better visualization\n",
    "        pos = nx.spring_layout(G, seed=42, k=1.5, iterations=50)\n",
    "        \n",
    "        # Highlight the target protein\n",
    "        node_colors = ['red' if node == protein else 'lightblue' for node in G.nodes()]\n",
    "        node_sizes = [2000 if node == protein else 800 for node in G.nodes()]\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.9, edgecolors='black', linewidths=2)\n",
    "        \n",
    "        # Draw edges (optionally weight by score)\n",
    "        edges = G.edges(data=True)\n",
    "        edge_widths = [data.get('weight', 1.0) * 2 for _, _, data in edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5, edge_color='gray')\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "        \n",
    "        plt.title(f\"{protein} {n_hops}-Hop Neighborhood Network\\n\"\n",
    "                 f\"({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)\",\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional visualization: Degree distribution\n",
    "        degrees = dict(G.degree())\n",
    "        degree_values = list(degrees.values())\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Degree distribution histogram\n",
    "        ax1.hist(degree_values, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        ax1.set_xlabel('Degree', fontsize=12)\n",
    "        ax1.set_ylabel('Number of Nodes', fontsize=12)\n",
    "        ax1.set_title(f'{protein} Network: Degree Distribution', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Top 10 nodes by degree\n",
    "        top_degree_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        nodes_list = [node for node, _ in top_degree_nodes]\n",
    "        degree_list = [deg for _, deg in top_degree_nodes]\n",
    "        \n",
    "        ax2.barh(range(len(nodes_list)), degree_list, color='coral', alpha=0.7)\n",
    "        ax2.set_yticks(range(len(nodes_list)))\n",
    "        ax2.set_yticklabels(nodes_list)\n",
    "        ax2.set_xlabel('Degree', fontsize=12)\n",
    "        ax2.set_title(f'{protein} Network: Top 10 Hubs', fontsize=14, fontweight='bold')\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Visualization Complete\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Centrality Analysis\n",
    "\n",
    "\n",
    "**Aim:** Compute graph centrality measures to identify key proteins (hubs) in the network.\n",
    "\n",
    "**Background:** Centrality measures quantify the importance of nodes in a network:\n",
    "- **Degree centrality**: Number of connections (simple but effective)\n",
    "- **Betweenness centrality**: How often a node lies on shortest paths between other nodes\n",
    "- **Closeness centrality**: Average distance to all other nodes\n",
    "\n",
    "**Biological Significance:** Hub proteins (high centrality) are often:\n",
    "- Essential for cellular function\n",
    "- Key regulators in signaling pathways\n",
    "- Important drug targets\n",
    "- Associated with disease when mutated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Graph Analysis and Centrality Measures\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 4: Graph Analysis and Centrality Measures\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # Store results for both graphs\n",
    "    analysis_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Basic graph statistics\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        components = list(nx.connected_components(G))\n",
    "        num_components = len(components)\n",
    "        is_connected = nx.is_connected(G)\n",
    "        \n",
    "        print(f\"\\nBasic Graph Statistics:\")\n",
    "        print(f\"  Number of nodes: {num_nodes}\")\n",
    "        print(f\"  Number of edges: {num_edges}\")\n",
    "        print(f\"  Number of connected components: {num_components}\")\n",
    "        print(f\"  Is connected: {is_connected}\")\n",
    "        if not is_connected:\n",
    "            largest_component = max(components, key=len)\n",
    "            print(f\"  Largest component size: {len(largest_component)} nodes\")\n",
    "        \n",
    "        # Calculate centrality measures\n",
    "        print(f\"\\nComputing centrality measures...\")\n",
    "        degree_cent = nx.degree_centrality(G)\n",
    "        betweenness_cent = nx.betweenness_centrality(G)\n",
    "        closeness_cent = nx.closeness_centrality(G)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        centrality_df = pd.DataFrame({\n",
    "            \"degree\": degree_cent,\n",
    "            \"betweenness\": betweenness_cent,\n",
    "            \"closeness\": closeness_cent\n",
    "        })\n",
    "        centrality_df.index.name = \"Protein\"\n",
    "        centrality_df = centrality_df.sort_values(\"degree\", ascending=False)\n",
    "        \n",
    "        analysis_results[protein] = {\n",
    "            'graph': G,\n",
    "            'centrality_df': centrality_df,\n",
    "            'stats': {\n",
    "                'nodes': num_nodes,\n",
    "                'edges': num_edges,\n",
    "                'components': num_components,\n",
    "                'is_connected': is_connected\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Centrality measures computed\")\n",
    "        print(f\"\\nTop 10 hub proteins (by degree centrality):\")\n",
    "        print(centrality_df.head(10).to_string())\n",
    "        \n",
    "        # Visualize centrality measures with bar plots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Degree centrality\n",
    "        top_degree = centrality_df.head(10)\n",
    "        axes[0].barh(range(len(top_degree)), top_degree['degree'], color='steelblue')\n",
    "        axes[0].set_yticks(range(len(top_degree)))\n",
    "        axes[0].set_yticklabels(top_degree.index)\n",
    "        axes[0].set_xlabel('Degree Centrality', fontsize=12)\n",
    "        axes[0].set_title(f'{protein}: Top 10 by Degree', fontsize=14, fontweight='bold')\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Betweenness centrality\n",
    "        top_between = centrality_df.sort_values(\"betweenness\", ascending=False).head(10)\n",
    "        axes[1].barh(range(len(top_between)), top_between['betweenness'], color='coral')\n",
    "        axes[1].set_yticks(range(len(top_between)))\n",
    "        axes[1].set_yticklabels(top_between.index)\n",
    "        axes[1].set_xlabel('Betweenness Centrality', fontsize=12)\n",
    "        axes[1].set_title(f'{protein}: Top 10 by Betweenness', fontsize=14, fontweight='bold')\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Closeness centrality\n",
    "        top_close = centrality_df.sort_values(\"closeness\", ascending=False).head(10)\n",
    "        axes[2].barh(range(len(top_close)), top_close['closeness'], color='mediumseagreen')\n",
    "        axes[2].set_yticks(range(len(top_close)))\n",
    "        axes[2].set_yticklabels(top_close.index)\n",
    "        axes[2].set_xlabel('Closeness Centrality', fontsize=12)\n",
    "        axes[2].set_title(f'{protein}: Top 10 by Closeness', fontsize=14, fontweight='bold')\n",
    "        axes[2].invert_yaxis()\n",
    "        axes[2].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Correlation between centrality measures\n",
    "        print(f\"\\nCorrelation between centrality measures ({protein}):\")\n",
    "        corr = centrality_df.corr()\n",
    "        print(corr.to_string())\n",
    "        \n",
    "        # Visualize network with top-centrality nodes highlighted\n",
    "        print(f\"\\nVisualizing network with top-centrality nodes highlighted...\")\n",
    "        \n",
    "        # Get top nodes for each centrality measure (top 5)\n",
    "        top_degree_nodes = set(centrality_df.sort_values(\"degree\", ascending=False).head(5).index)\n",
    "        top_between_nodes = set(centrality_df.sort_values(\"betweenness\", ascending=False).head(5).index)\n",
    "        top_close_nodes = set(centrality_df.sort_values(\"closeness\", ascending=False).head(5).index)\n",
    "        \n",
    "        # Create three visualizations, one for each centrality measure\n",
    "        centrality_types = [\n",
    "            (\"degree\", top_degree_nodes, \"Degree Centrality\", \"steelblue\"),\n",
    "            (\"betweenness\", top_between_nodes, \"Betweenness Centrality\", \"coral\"),\n",
    "            (\"closeness\", top_close_nodes, \"Closeness Centrality\", \"mediumseagreen\")\n",
    "        ]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "        \n",
    "        # Use consistent layout for all three plots\n",
    "        pos = nx.spring_layout(G, seed=42, k=1.5, iterations=50)\n",
    "        \n",
    "        for idx, (cent_type, top_nodes, cent_name, color) in enumerate(centrality_types):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Color nodes: red for target protein, color for top centrality nodes, lightblue for others\n",
    "            node_colors = []\n",
    "            node_sizes = []\n",
    "            for node in G.nodes():\n",
    "                if node == protein:\n",
    "                    node_colors.append('red')\n",
    "                    node_sizes.append(2000)\n",
    "                elif node in top_nodes:\n",
    "                    node_colors.append(color)\n",
    "                    node_sizes.append(1500)\n",
    "                else:\n",
    "                    node_colors.append('lightblue')\n",
    "                    node_sizes.append(600)\n",
    "            \n",
    "            # Draw network\n",
    "            nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                                  alpha=0.9, edgecolors='black', linewidths=1.5, ax=ax)\n",
    "            nx.draw_networkx_edges(G, pos, width=1, alpha=0.3, edge_color='gray', ax=ax)\n",
    "            nx.draw_networkx_labels(G, pos, font_size=6, font_weight='bold', ax=ax)\n",
    "            \n",
    "            ax.set_title(f'{protein}: Top 5 by {cent_name}', fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top nodes for each centrality\n",
    "        print(f\"\\nTop 5 nodes by centrality ({protein}):\")\n",
    "        print(f\"  Degree: {', '.join(list(top_degree_nodes))}\")\n",
    "        print(f\"  Betweenness: {', '.join(list(top_between_nodes))}\")\n",
    "        print(f\"  Closeness: {', '.join(list(top_close_nodes))}\")\n",
    "    \n",
    "    # Compare results between PARG and TP53\n",
    "    if len(analysis_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: PARG vs TP53\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        parg_stats = analysis_results['PARG']['stats']\n",
    "        tp53_stats = analysis_results['TP53']['stats']\n",
    "        \n",
    "        print(f\"\\nGraph Size:\")\n",
    "        print(f\"  PARG: {parg_stats['nodes']} nodes, {parg_stats['edges']} edges\")\n",
    "        print(f\"  TP53: {tp53_stats['nodes']} nodes, {tp53_stats['edges']} edges\")\n",
    "        \n",
    "        print(f\"\\nConnectivity:\")\n",
    "        print(f\"  PARG: {parg_stats['components']} component(s), connected: {parg_stats['is_connected']}\")\n",
    "        print(f\"  TP53: {tp53_stats['components']} component(s), connected: {tp53_stats['is_connected']}\")\n",
    "        \n",
    "        # Compare average centrality values\n",
    "        parg_cent = analysis_results['PARG']['centrality_df']\n",
    "        tp53_cent = analysis_results['TP53']['centrality_df']\n",
    "        \n",
    "        print(f\"\\nAverage Centrality Values:\")\n",
    "        print(f\"  PARG - Degree: {parg_cent['degree'].mean():.4f}, Betweenness: {parg_cent['betweenness'].mean():.4f}, Closeness: {parg_cent['closeness'].mean():.4f}\")\n",
    "        print(f\"  TP53 - Degree: {tp53_cent['degree'].mean():.4f}, Betweenness: {tp53_cent['betweenness'].mean():.4f}, Closeness: {tp53_cent['closeness'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Community Detection (Louvain Clustering)\n",
    "\n",
    "**Aim:** Identify functional communities/modules in both PARG and TP53 networks using the Louvain algorithm. Compare the community structure between the two networks.\n",
    "\n",
    "**Background:** The Louvain algorithm is a community detection method that optimizes modularity. It identifies densely connected groups of nodes that are sparsely connected to other groups. Modularity measures the quality of a partition by comparing the number of edges within communities to what would be expected in a random network.\n",
    "\n",
    "**Biological Significance:** Communities in PPI networks often correspond to:\n",
    "- Protein complexes (e.g., ribosome, proteasome)\n",
    "- Functional pathways (e.g., cell cycle, apoptosis)\n",
    "- Signaling cascades\n",
    "- Co-regulated gene products\n",
    "\n",
    "**Key Concepts:**\n",
    "- Community detection algorithms\n",
    "- Modularity optimization\n",
    "- Network partitioning\n",
    "- Comparing community structures across networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Community Detection using Louvain algorithm\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 5: Community Detection (Louvain Clustering)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if Louvain is available\n",
    "try:\n",
    "    import community.community_louvain as community_louvain\n",
    "    louvain_available = True\n",
    "except ImportError:\n",
    "    louvain_available = False\n",
    "    print(\"⚠ python-louvain not installed. Using NetworkX alternative method...\")\n",
    "\n",
    "# Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # Store clustering results\n",
    "    clustering_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Community Detection for {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if louvain_available:\n",
    "            print(\"Performing Louvain clustering...\")\n",
    "            \n",
    "            # Compute best partition\n",
    "            partition = community_louvain.best_partition(G)\n",
    "            \n",
    "            # Organize proteins by cluster\n",
    "            clusters = defaultdict(list)\n",
    "            for node, cluster_id in partition.items():\n",
    "                clusters[cluster_id].append(node)\n",
    "            \n",
    "            print(f\"✓ Identified {len(clusters)} communities\")\n",
    "            print(f\"\\nCommunity sizes:\")\n",
    "            for cid, members in sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "                print(f\"  Cluster {cid}: {len(members)} proteins\")\n",
    "                if len(members) <= 10:  # Show members for small clusters\n",
    "                    print(f\"    Members: {', '.join(members)}\")\n",
    "            \n",
    "            # Calculate modularity\n",
    "            modularity = community_louvain.modularity(partition, G)\n",
    "            print(f\"\\nNetwork modularity: {modularity:.4f}\")\n",
    "            print(\"(Higher modularity indicates better community structure)\")\n",
    "            \n",
    "            clustering_results[protein] = {\n",
    "                'partition': partition,\n",
    "                'clusters': clusters,\n",
    "                'modularity': modularity\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Fallback: Use NetworkX built-in community detection\n",
    "            print(\"Using NetworkX greedy modularity communities...\")\n",
    "            communities = nx.community.greedy_modularity_communities(G)\n",
    "            \n",
    "            print(f\"✓ Identified {len(communities)} communities\")\n",
    "            for i, comm in enumerate(sorted(communities, key=len, reverse=True), 1):\n",
    "                print(f\"  Community {i}: {len(comm)} proteins\")\n",
    "                if len(comm) <= 10:\n",
    "                    print(f\"    Members: {', '.join(sorted(comm))}\")\n",
    "            \n",
    "            # Create partition dict for consistency\n",
    "            partition = {}\n",
    "            for i, comm in enumerate(communities):\n",
    "                for node in comm:\n",
    "                    partition[node] = i\n",
    "            \n",
    "            # Calculate modularity using NetworkX\n",
    "            modularity = nx.community.modularity(G, communities)\n",
    "            print(f\"\\nNetwork modularity: {modularity:.4f}\")\n",
    "            \n",
    "            # Convert to clusters dict format\n",
    "            clusters = defaultdict(list)\n",
    "            for node, cluster_id in partition.items():\n",
    "                clusters[cluster_id].append(node)\n",
    "            \n",
    "            clustering_results[protein] = {\n",
    "                'partition': partition,\n",
    "                'clusters': clusters,\n",
    "                'modularity': modularity\n",
    "            }\n",
    "        \n",
    "        # Visualize network with communities\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Use consistent layout\n",
    "        pos = nx.spring_layout(G, seed=42, k=1.5, iterations=50)\n",
    "        \n",
    "        # Color nodes by community\n",
    "        node_colors = [partition[node] for node in G.nodes()]\n",
    "        cmap = plt.cm.tab20\n",
    "        \n",
    "        # Highlight target protein\n",
    "        node_sizes = [2000 if node == protein else 1200 for node in G.nodes()]\n",
    "        node_edge_colors = ['red' if node == protein else 'black' for node in G.nodes()]\n",
    "        node_linewidths = [3 if node == protein else 1.5 for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, \n",
    "                               cmap=cmap, alpha=0.9, edgecolors=node_edge_colors, \n",
    "                               linewidths=node_linewidths)\n",
    "        nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.4, edge_color='gray')\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "        \n",
    "        plt.title(f\"{protein} Network with Communities (Louvain, {len(clusters)} clusters, modularity={modularity:.3f})\", \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Compare community structures between PARG and TP53\n",
    "    if len(clustering_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: PARG vs TP53 Communities\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        parg_clusters = clustering_results['PARG']['clusters']\n",
    "        tp53_clusters = clustering_results['TP53']['clusters']\n",
    "        \n",
    "        print(f\"\\nNumber of Communities:\")\n",
    "        print(f\"  PARG: {len(parg_clusters)} communities\")\n",
    "        print(f\"  TP53: {len(tp53_clusters)} communities\")\n",
    "        \n",
    "        print(f\"\\nModularity:\")\n",
    "        print(f\"  PARG: {clustering_results['PARG']['modularity']:.4f}\")\n",
    "        print(f\"  TP53: {clustering_results['TP53']['modularity']:.4f}\")\n",
    "        \n",
    "        # Compare community sizes\n",
    "        parg_sizes = [len(members) for members in parg_clusters.values()]\n",
    "        tp53_sizes = [len(members) for members in tp53_clusters.values()]\n",
    "        \n",
    "        print(f\"\\nCommunity Size Statistics:\")\n",
    "        print(f\"  PARG - Mean: {np.mean(parg_sizes):.2f}, Max: {max(parg_sizes)}, Min: {min(parg_sizes)}\")\n",
    "        print(f\"  TP53 - Mean: {np.mean(tp53_sizes):.2f}, Max: {max(tp53_sizes)}, Min: {min(tp53_sizes)}\")\n",
    "        \n",
    "        # Find common proteins in communities\n",
    "        parg_all_nodes = set()\n",
    "        for members in parg_clusters.values():\n",
    "            parg_all_nodes.update(members)\n",
    "        \n",
    "        tp53_all_nodes = set()\n",
    "        for members in tp53_clusters.values():\n",
    "            tp53_all_nodes.update(members)\n",
    "        \n",
    "        common_nodes = parg_all_nodes & tp53_all_nodes\n",
    "        print(f\"\\nCommon nodes in both networks: {len(common_nodes)}\")\n",
    "        \n",
    "        if len(common_nodes) > 0:\n",
    "            # Check if common nodes are in similar communities\n",
    "            print(f\"  Examples: {', '.join(list(common_nodes)[:10])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Functional Enrichment Analysis (g:Profiler)\n",
    "\n",
    "**Aim:** Query g:Profiler API to identify enriched Gene Ontology (GO) terms and KEGG pathways for proteins in both PARG and TP53 networks. Compare the functional enrichment between the two networks to understand their biological roles.\n",
    "\n",
    "**Background:** Functional enrichment analysis identifies which biological processes, molecular functions, cellular components, or pathways are overrepresented in a set of genes/proteins compared to the background. g:Profiler is a web-based tool that performs enrichment analysis using multiple databases including:\n",
    "- **GO:BP** (Gene Ontology Biological Process): Biological processes proteins are involved in\n",
    "- **GO:MF** (Gene Ontology Molecular Function): Molecular functions proteins perform\n",
    "- **GO:CC** (Gene Ontology Cellular Component): Cellular locations where proteins are found\n",
    "- **KEGG**: Kyoto Encyclopedia of Genes and Genomes pathways\n",
    "- **REAC**: Reactome pathways\n",
    "\n",
    "**Biological Significance:** Enrichment analysis helps interpret network data by:\n",
    "- Identifying common biological functions of interacting proteins\n",
    "- Revealing pathways that are overrepresented in the network\n",
    "- Understanding the biological context of protein interactions\n",
    "- Comparing functional profiles between different networks\n",
    "\n",
    "**Key Concepts:**\n",
    "- Functional enrichment analysis\n",
    "- Statistical significance (p-values, FDR correction)\n",
    "- Gene Ontology and pathway databases\n",
    "- Interpreting enrichment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Functional Enrichment Analysis using g:Profiler\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 6: Functional Enrichment Analysis (g:Profiler)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # Store enrichment results\n",
    "    enrichment_results = {}\n",
    "    \n",
    "    # Query g:Profiler API\n",
    "    url = \"https://biit.cs.ut.ee/gprofiler/api/gost/profile/\"\n",
    "    \n",
    "    # Color map for different sources\n",
    "    source_colors = {\n",
    "        'GO:BP': 'steelblue',\n",
    "        'GO:MF': 'coral',\n",
    "        'GO:CC': 'mediumseagreen',\n",
    "        'KEGG': 'purple',\n",
    "        'REAC': 'orange'\n",
    "    }\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        genes = list(G.nodes())\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Enrichment Analysis for {protein} Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Querying g:Profiler for {len(genes)} genes...\")\n",
    "        \n",
    "        payload = {\n",
    "            \"organism\": \"hsapiens\",\n",
    "            \"query\": genes,\n",
    "            \"sources\": [\"GO:BP\", \"GO:MF\", \"GO:CC\", \"KEGG\", \"REAC\"],\n",
    "            \"user_threshold\": 0.05,  # P-value threshold\n",
    "            \"all_results\": False,\n",
    "            \"ordered\": True\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            results = response.json()\n",
    "            \n",
    "            # Parse and display top enriched terms\n",
    "            if 'result' in results and len(results['result']) > 0:\n",
    "                enrichment_df = pd.DataFrame(results['result'])\n",
    "                \n",
    "                # Filter by p-value and sort\n",
    "                enrichment_df = enrichment_df[enrichment_df['p_value'] < 0.05].copy()\n",
    "                enrichment_df = enrichment_df.sort_values('p_value')\n",
    "                \n",
    "                enrichment_results[protein] = enrichment_df\n",
    "                \n",
    "                print(f\"✓ Found {len(enrichment_df)} significantly enriched terms (p < 0.05)\")\n",
    "                print(f\"\\nTop 10 enriched terms:\")\n",
    "                top_10 = enrichment_df.head(10)\n",
    "                print(top_10[['native', 'name', 'p_value', 'source']].to_string(index=False))\n",
    "                \n",
    "                # Visualize top enriched terms\n",
    "                top_terms = enrichment_df.head(15)\n",
    "                \n",
    "                if len(top_terms) > 0:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    \n",
    "                    y_pos = np.arange(len(top_terms))\n",
    "                    colors_list = [source_colors.get(src, 'gray') for src in top_terms['source']]\n",
    "                    \n",
    "                    # Plot -log10(p-value)\n",
    "                    neg_log_p = -np.log10(top_terms['p_value'])\n",
    "                    bars = ax.barh(y_pos, neg_log_p, color=colors_list, alpha=0.8, edgecolor='black')\n",
    "                    \n",
    "                    ax.set_yticks(y_pos)\n",
    "                    ax.set_yticklabels([name[:60] + '...' if len(name) > 60 else name \n",
    "                                        for name in top_terms['name']], fontsize=10)\n",
    "                    ax.set_xlabel('-log10(p-value)', fontsize=12, fontweight='bold')\n",
    "                    ax.set_title(f'{protein} Network: Top 15 Enriched Terms', fontsize=14, fontweight='bold')\n",
    "                    ax.grid(axis='x', alpha=0.3)\n",
    "                    ax.invert_yaxis()\n",
    "                    \n",
    "                    # Add legend\n",
    "                    from matplotlib.patches import Patch\n",
    "                    legend_elements = [Patch(facecolor=color, label=source) \n",
    "                                      for source, color in source_colors.items()]\n",
    "                    ax.legend(handles=legend_elements, loc='lower right')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Breakdown by source\n",
    "                print(f\"\\nEnrichment by source ({protein}):\")\n",
    "                source_counts = enrichment_df['source'].value_counts()\n",
    "                for source, count in source_counts.items():\n",
    "                    print(f\"  {source}: {count} enriched terms\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ No enrichment results found\")\n",
    "                enrichment_results[protein] = pd.DataFrame()\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"✗ Error querying g:Profiler: {e}\")\n",
    "            print(\"Note: This may be due to network issues or API rate limits.\")\n",
    "            enrichment_results[protein] = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing enrichment results: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            enrichment_results[protein] = pd.DataFrame()\n",
    "    \n",
    "    # Compare enrichment between PARG and TP53\n",
    "    if len(enrichment_results) == 2:\n",
    "        parg_df = enrichment_results['PARG']\n",
    "        tp53_df = enrichment_results['TP53']\n",
    "        \n",
    "        if len(parg_df) > 0 and len(tp53_df) > 0:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"Comparison: PARG vs TP53 Enrichment\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            print(f\"\\nNumber of enriched terms:\")\n",
    "            print(f\"  PARG: {len(parg_df)} terms\")\n",
    "            print(f\"  TP53: {len(tp53_df)} terms\")\n",
    "            \n",
    "            # Find common enriched terms\n",
    "            parg_terms = set(parg_df['native'])\n",
    "            tp53_terms = set(tp53_df['native'])\n",
    "            common_terms = parg_terms & tp53_terms\n",
    "            \n",
    "            print(f\"\\nCommon enriched terms: {len(common_terms)}\")\n",
    "            if len(common_terms) > 0:\n",
    "                # Show common terms with their p-values\n",
    "                common_parg = parg_df[parg_df['native'].isin(common_terms)][['native', 'name', 'p_value', 'source']]\n",
    "                common_tp53 = tp53_df[tp53_df['native'].isin(common_terms)][['native', 'name', 'p_value', 'source']]\n",
    "                \n",
    "                print(f\"\\nTop 10 common enriched terms:\")\n",
    "                # Merge to show both p-values\n",
    "                merged = common_parg.merge(common_tp53, on='native', suffixes=('_PARG', '_TP53'))\n",
    "                merged = merged.sort_values('p_value_PARG').head(10)\n",
    "                print(merged[['native', 'name_PARG', 'p_value_PARG', 'p_value_TP53', 'source_PARG']].to_string(index=False))\n",
    "            \n",
    "            # Compare by source\n",
    "            print(f\"\\nEnrichment by source:\")\n",
    "            print(f\"  PARG:\")\n",
    "            for source, count in parg_df['source'].value_counts().items():\n",
    "                print(f\"    {source}: {count}\")\n",
    "            print(f\"  TP53:\")\n",
    "            for source, count in tp53_df['source'].value_counts().items():\n",
    "                print(f\"    {source}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Simulated Network Attack (Protein Inhibition)\n",
    "\n",
    "**Aim:** Simulate the effect of inhibiting the central protein (TP53 or PARG) by removing it from each network. Compare graph properties before and after removal to understand the impact of targeted protein inhibition on network structure and connectivity.\n",
    "\n",
    "**Background:** In drug discovery, targeted protein inhibition is a common therapeutic strategy. Understanding how removing a specific protein affects the network structure helps predict:\n",
    "- Network robustness and resilience\n",
    "- Potential side effects of inhibition\n",
    "- Alternative pathways that may compensate\n",
    "- Critical nodes whose removal disrupts the network\n",
    "\n",
    "This simulation mimics what happens when a selective inhibitor blocks the function of the target protein, effectively removing it from the interaction network.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Network robustness and vulnerability\n",
    "- Targeted node removal\n",
    "- Graph connectivity and fragmentation\n",
    "- Network metrics (density, clustering, path length)\n",
    "- Largest connected component analysis\n",
    "- Biological implications of hub protein inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Simulated Network Attack (Removing Central Proteins)\n",
    "print(\"=\" * 60)\n",
    "print(\"Task 7: Simulated Network Attack (Protein Inhibition)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if graphs were created in Task 2\n",
    "if 'graphs' not in locals() or len(graphs) == 0:\n",
    "    print(\"⚠ Warning: Graphs not found. Please run Task 2 first.\")\n",
    "else:\n",
    "    # Store attack results\n",
    "    attack_results = {}\n",
    "    \n",
    "    for protein in [\"PARG\", \"TP53\"]:\n",
    "        if protein not in graphs or graphs[protein].number_of_nodes() == 0:\n",
    "            print(f\"⚠ Warning: No graph available for {protein}\")\n",
    "            continue\n",
    "        \n",
    "        G = graphs[protein]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Simulating Attack: Removing {protein} from Network\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Calculate metrics BEFORE removal\n",
    "        print(f\"\\nBEFORE removal of {protein}:\")\n",
    "        before_metrics = {\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'density': nx.density(G),\n",
    "            'is_connected': nx.is_connected(G),\n",
    "            'components': len(list(nx.connected_components(G))),\n",
    "            'largest_component': len(max(nx.connected_components(G), key=len)) if G.number_of_nodes() > 0 else 0,\n",
    "            'avg_degree': sum(dict(G.degree()).values()) / G.number_of_nodes() if G.number_of_nodes() > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        if before_metrics['is_connected']:\n",
    "            before_metrics['avg_path_length'] = nx.average_shortest_path_length(G)\n",
    "            before_metrics['diameter'] = nx.diameter(G)\n",
    "        else:\n",
    "            before_metrics['avg_path_length'] = None\n",
    "            before_metrics['diameter'] = None\n",
    "        \n",
    "        before_metrics['clustering'] = nx.average_clustering(G)\n",
    "        \n",
    "        print(f\"  Nodes: {before_metrics['nodes']}\")\n",
    "        print(f\"  Edges: {before_metrics['edges']}\")\n",
    "        print(f\"  Density: {before_metrics['density']:.4f}\")\n",
    "        print(f\"  Is connected: {before_metrics['is_connected']}\")\n",
    "        print(f\"  Connected components: {before_metrics['components']}\")\n",
    "        print(f\"  Largest component size: {before_metrics['largest_component']}\")\n",
    "        print(f\"  Average degree: {before_metrics['avg_degree']:.2f}\")\n",
    "        print(f\"  Average clustering: {before_metrics['clustering']:.4f}\")\n",
    "        if before_metrics['avg_path_length']:\n",
    "            print(f\"  Average path length: {before_metrics['avg_path_length']:.3f}\")\n",
    "            print(f\"  Diameter: {before_metrics['diameter']}\")\n",
    "        \n",
    "        # Remove the central protein\n",
    "        G_attacked = G.copy()\n",
    "        if protein in G_attacked.nodes():\n",
    "            G_attacked.remove_node(protein)\n",
    "            print(f\"\\n✓ Removed {protein} from network\")\n",
    "        else:\n",
    "            print(f\"⚠ Warning: {protein} not found in graph nodes\")\n",
    "        \n",
    "        # Calculate metrics AFTER removal\n",
    "        print(f\"\\nAFTER removal of {protein}:\")\n",
    "        after_metrics = {\n",
    "            'nodes': G_attacked.number_of_nodes(),\n",
    "            'edges': G_attacked.number_of_edges(),\n",
    "            'density': nx.density(G_attacked) if G_attacked.number_of_nodes() > 0 else 0,\n",
    "            'is_connected': nx.is_connected(G_attacked) if G_attacked.number_of_nodes() > 0 else False,\n",
    "            'components': len(list(nx.connected_components(G_attacked))) if G_attacked.number_of_nodes() > 0 else 0,\n",
    "            'largest_component': len(max(nx.connected_components(G_attacked), key=len)) if G_attacked.number_of_nodes() > 0 else 0,\n",
    "            'avg_degree': sum(dict(G_attacked.degree()).values()) / G_attacked.number_of_nodes() if G_attacked.number_of_nodes() > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        if after_metrics['is_connected'] and G_attacked.number_of_nodes() > 1:\n",
    "            after_metrics['avg_path_length'] = nx.average_shortest_path_length(G_attacked)\n",
    "            after_metrics['diameter'] = nx.diameter(G_attacked)\n",
    "        else:\n",
    "            after_metrics['avg_path_length'] = None\n",
    "            after_metrics['diameter'] = None\n",
    "        \n",
    "        after_metrics['clustering'] = nx.average_clustering(G_attacked) if G_attacked.number_of_nodes() > 0 else 0\n",
    "        \n",
    "        print(f\"  Nodes: {after_metrics['nodes']} (lost {before_metrics['nodes'] - after_metrics['nodes']})\")\n",
    "        print(f\"  Edges: {after_metrics['edges']} (lost {before_metrics['edges'] - after_metrics['edges']})\")\n",
    "        print(f\"  Density: {after_metrics['density']:.4f} (change: {after_metrics['density'] - before_metrics['density']:+.4f})\")\n",
    "        print(f\"  Is connected: {after_metrics['is_connected']}\")\n",
    "        print(f\"  Connected components: {after_metrics['components']} (change: {after_metrics['components'] - before_metrics['components']:+d})\")\n",
    "        print(f\"  Largest component size: {after_metrics['largest_component']} (lost {before_metrics['largest_component'] - after_metrics['largest_component']} nodes)\")\n",
    "        print(f\"  Average degree: {after_metrics['avg_degree']:.2f} (change: {after_metrics['avg_degree'] - before_metrics['avg_degree']:+.2f})\")\n",
    "        print(f\"  Average clustering: {after_metrics['clustering']:.4f} (change: {after_metrics['clustering'] - before_metrics['clustering']:+.4f})\")\n",
    "        if after_metrics['avg_path_length']:\n",
    "            print(f\"  Average path length: {after_metrics['avg_path_length']:.3f}\")\n",
    "            if before_metrics['avg_path_length']:\n",
    "                print(f\"    (change: {after_metrics['avg_path_length'] - before_metrics['avg_path_length']:+.3f})\")\n",
    "            print(f\"  Diameter: {after_metrics['diameter']}\")\n",
    "        \n",
    "        # Calculate impact metrics\n",
    "        impact = {\n",
    "            'nodes_lost': before_metrics['nodes'] - after_metrics['nodes'],\n",
    "            'edges_lost': before_metrics['edges'] - after_metrics['edges'],\n",
    "            'largest_component_loss': before_metrics['largest_component'] - after_metrics['largest_component'],\n",
    "            'largest_component_retention': after_metrics['largest_component'] / before_metrics['largest_component'] if before_metrics['largest_component'] > 0 else 0,\n",
    "            'fragmentation': after_metrics['components'] - before_metrics['components'],\n",
    "            'connectivity_lost': not after_metrics['is_connected'] if before_metrics['is_connected'] else False\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nImpact Summary:\")\n",
    "        print(f\"  Nodes lost: {impact['nodes_lost']} ({impact['nodes_lost']/before_metrics['nodes']*100:.1f}%)\")\n",
    "        print(f\"  Edges lost: {impact['edges_lost']} ({impact['edges_lost']/before_metrics['edges']*100:.1f}%)\")\n",
    "        print(f\"  Largest component retention: {impact['largest_component_retention']*100:.1f}%\")\n",
    "        print(f\"  Network fragmentation: {impact['fragmentation']} additional components\")\n",
    "        if impact['connectivity_lost']:\n",
    "            print(f\"  ⚠ Network connectivity LOST (was connected, now fragmented)\")\n",
    "        elif not after_metrics['is_connected']:\n",
    "            print(f\"  Network remains disconnected (was already disconnected)\")\n",
    "        else:\n",
    "            print(f\"  ✓ Network remains connected\")\n",
    "        \n",
    "        attack_results[protein] = {\n",
    "            'before': before_metrics,\n",
    "            'after': after_metrics,\n",
    "            'impact': impact,\n",
    "            'graph_attacked': G_attacked\n",
    "        }\n",
    "        \n",
    "        # Visualize before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Before removal\n",
    "        pos_before = nx.spring_layout(G, seed=42, k=1.5, iterations=50)\n",
    "        node_colors_before = ['red' if node == protein else 'lightblue' for node in G.nodes()]\n",
    "        node_sizes_before = [2000 if node == protein else 800 for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos_before, node_color=node_colors_before, \n",
    "                               node_size=node_sizes_before, alpha=0.9, \n",
    "                               edgecolors='black', linewidths=2, ax=axes[0])\n",
    "        nx.draw_networkx_edges(G, pos_before, width=1, alpha=0.4, \n",
    "                               edge_color='gray', ax=axes[0])\n",
    "        nx.draw_networkx_labels(G, pos_before, font_size=7, font_weight='bold', ax=axes[0])\n",
    "        axes[0].set_title(f'{protein} Network BEFORE Removal\\n'\n",
    "                         f'({before_metrics[\"nodes\"]} nodes, {before_metrics[\"edges\"]} edges, '\n",
    "                         f'connected: {before_metrics[\"is_connected\"]})',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # After removal\n",
    "        if G_attacked.number_of_nodes() > 0:\n",
    "            pos_after = nx.spring_layout(G_attacked, seed=42, k=1.5, iterations=50)\n",
    "            node_colors_after = ['lightblue' for node in G_attacked.nodes()]\n",
    "            node_sizes_after = [800 for node in G_attacked.nodes()]\n",
    "            \n",
    "            nx.draw_networkx_nodes(G_attacked, pos_after, node_color=node_colors_after,\n",
    "                                  node_size=node_sizes_after, alpha=0.9,\n",
    "                                  edgecolors='black', linewidths=1.5, ax=axes[1])\n",
    "            nx.draw_networkx_edges(G_attacked, pos_after, width=1, alpha=0.4,\n",
    "                                  edge_color='gray', ax=axes[1])\n",
    "            nx.draw_networkx_labels(G_attacked, pos_after, font_size=7, \n",
    "                                    font_weight='bold', ax=axes[1])\n",
    "            axes[1].set_title(f'{protein} Network AFTER Removal\\n'\n",
    "                             f'({after_metrics[\"nodes\"]} nodes, {after_metrics[\"edges\"]} edges, '\n",
    "                             f'connected: {after_metrics[\"is_connected\"]}, '\n",
    "                             f'{after_metrics[\"components\"]} components)',\n",
    "                             fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Network completely\\ndisconnected', \n",
    "                        ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_title(f'{protein} Network AFTER Removal\\n(No nodes remaining)',\n",
    "                             fontsize=12, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Bar plot comparing metrics\n",
    "        metrics_to_compare = ['nodes', 'edges', 'density', 'avg_degree', 'clustering']\n",
    "        before_values = [before_metrics[m] for m in metrics_to_compare]\n",
    "        after_values = [after_metrics[m] for m in metrics_to_compare]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(metrics_to_compare))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, before_values, width, label='Before', color='steelblue', alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, after_values, width, label='After', color='coral', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Value', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{protein} Network: Metrics Before vs After Removal', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics_to_compare, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Compare impact between PARG and TP53 removal\n",
    "    if len(attack_results) == 2:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Comparison: Impact of Removing PARG vs TP53\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        parg_impact = attack_results['PARG']['impact']\n",
    "        tp53_impact = attack_results['TP53']['impact']\n",
    "        \n",
    "        print(f\"\\nNetwork Fragmentation:\")\n",
    "        print(f\"  PARG removal: {parg_impact['fragmentation']} additional components\")\n",
    "        print(f\"  TP53 removal: {tp53_impact['fragmentation']} additional components\")\n",
    "        \n",
    "        print(f\"\\nLargest Component Retention:\")\n",
    "        print(f\"  PARG removal: {parg_impact['largest_component_retention']*100:.1f}%\")\n",
    "        print(f\"  TP53 removal: {tp53_impact['largest_component_retention']*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nConnectivity Loss:\")\n",
    "        print(f\"  PARG removal: {'Network disconnected' if parg_impact['connectivity_lost'] else 'Network remains connected'}\")\n",
    "        print(f\"  TP53 removal: {'Network disconnected' if tp53_impact['connectivity_lost'] else 'Network remains connected'}\")\n",
    "        \n",
    "        print(f\"\\nEdges Lost:\")\n",
    "        print(f\"  PARG removal: {parg_impact['edges_lost']} edges ({parg_impact['edges_lost']/attack_results['PARG']['before']['edges']*100:.1f}%)\")\n",
    "        print(f\"  TP53 removal: {tp53_impact['edges_lost']} edges ({tp53_impact['edges_lost']/attack_results['TP53']['before']['edges']*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
